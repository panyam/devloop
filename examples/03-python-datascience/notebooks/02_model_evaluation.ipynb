{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation Notebook\n",
    "\n",
    "This notebook demonstrates model evaluation and analysis after training.\n",
    "\n",
    "Changes to this notebook will trigger devloop to restart Jupyter Lab automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import yaml\n",
    "import sys\n",
    "import os\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, os.path.join('..', 'src'))\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Trained Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "try:\n",
    "    model = joblib.load('../models/model.pkl')\n",
    "    print(\"âœ… Model loaded successfully!\")\nexcept FileNotFoundError:\n",
    "    print(\"âŒ Model not found. Please run the training script first.\")\n",
    "    print(\"Run: python src/train.py --config configs/model.yaml\")\n",
    "    model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training metrics\n",
    "try:\n",
    "    with open('../models/metrics.yaml', 'r') as f:\n",
    "        metrics = yaml.safe_load(f)\n",
    "    print(\"Training Metrics:\")\n",
    "    for key, value in metrics.items():\n",
    "        if key != 'model_params':\n",
    "            print(f\"  {key}: {value}\")\nexcept FileNotFoundError:\n",
    "    print(\"No metrics file found.\")\n",
    "    metrics = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test data for evaluation (in real scenario, this would be your test set)\n",
    "from train import generate_synthetic_data, preprocess_data\n",
    "\n",
    "# Generate evaluation dataset\n",
    "eval_df = generate_synthetic_data()\n",
    "X_eval, y_eval = preprocess_data(eval_df, {})\n",
    "\n",
    "print(f\"Evaluation data shape: X={X_eval.shape}, y={y_eval.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model is not None:\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_eval)\n",
    "    y_pred_proba = model.predict_proba(X_eval)[:, 1]  # Probability of positive class\n",
    "    \n",
    "    # Calculate metrics\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "    \n",
    "    accuracy = accuracy_score(y_eval, y_pred)\n",
    "    precision = precision_score(y_eval, y_pred)\n",
    "    recall = recall_score(y_eval, y_pred)\n",
    "    f1 = f1_score(y_eval, y_pred)\n",
    "    \n",
    "    print(\"Model Performance on Evaluation Set:\")\n",
    "    print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall:    {recall:.4f}\")\n",
    "    print(f\"  F1-Score:  {f1:.4f}\")\nelse:\n",
    "    print(\"Cannot evaluate model - model not loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model is not None:\n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(y_eval, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Negative', 'Positive'],\n",
    "                yticklabels=['Negative', 'Positive'])\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate confusion matrix percentages\n",
    "    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "    print(\"\\nConfusion Matrix (Percentages):\")\n",
    "    print(f\"True Negatives:  {cm_percent[0,0]:.1f}%\")\n",
    "    print(f\"False Positives: {cm_percent[0,1]:.1f}%\")\n",
    "    print(f\"False Negatives: {cm_percent[1,0]:.1f}%\")\n",
    "    print(f\"True Positives:  {cm_percent[1,1]:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model is not None:\n",
    "    # Calculate ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_eval, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "             label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', \n",
    "             label='Random classifier')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Area Under the Curve (AUC): {roc_auc:.4f}\")\n",
    "    \n",
    "    # Interpretation\n",
    "    if roc_auc > 0.9:\n",
    "        interpretation = \"Excellent\"\n",
    "    elif roc_auc > 0.8:\n",
    "        interpretation = \"Good\"\n",
    "    elif roc_auc > 0.7:\n",
    "        interpretation = \"Fair\"\n",
    "    elif roc_auc > 0.6:\n",
    "        interpretation = \"Poor\"\n",
    "    else:\n",
    "        interpretation = \"Very Poor\"\n",
    "    \n",
    "    print(f\"Model Performance: {interpretation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model is not None and hasattr(model, 'feature_importances_'):\n",
    "    # Get feature importances\n",
    "    importances = model.feature_importances_\n",
    "    feature_names = X_eval.columns\n",
    "    \n",
    "    # Create DataFrame for easier handling\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importances\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"Top 10 Most Important Features:\")\n",
    "    print(feature_importance_df.head(10))\n",
    "    \n",
    "    # Plot feature importances\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    top_features = feature_importance_df.head(10)\n",
    "    plt.barh(range(len(top_features)), top_features['importance'])\n",
    "    plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title('Top 10 Feature Importances')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\nelse:\n",
    "    print(\"Feature importance not available for this model type.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model is not None:\n",
    "    # Analyze prediction probabilities\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Plot 1: Prediction probability distribution\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(y_pred_proba[y_eval == 0], bins=30, alpha=0.7, label='Class 0 (Negative)', density=True)\n",
    "    plt.hist(y_pred_proba[y_eval == 1], bins=30, alpha=0.7, label='Class 1 (Positive)', density=True)\n",
    "    plt.xlabel('Prediction Probability')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title('Prediction Probability Distribution by True Class')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Calibration plot (reliability diagram)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    from sklearn.calibration import calibration_curve\n",
    "    \n",
    "    prob_true, prob_pred = calibration_curve(y_eval, y_pred_proba, n_bins=10)\n",
    "    plt.plot(prob_pred, prob_true, marker='o', linewidth=2, label='Model')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', label='Perfect calibration')\n",
    "    plt.xlabel('Mean Predicted Probability')\n",
    "    plt.ylabel('Fraction of Positives')\n",
    "    plt.title('Calibration Plot')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison with Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model is not None:\n",
    "    # Compare with simple baselines\n",
    "    from sklearn.dummy import DummyClassifier\n",
    "    \n",
    "    # Majority class baseline\n",
    "    dummy_majority = DummyClassifier(strategy='most_frequent')\n",
    "    dummy_majority.fit(X_eval, y_eval)\n",
    "    dummy_pred_majority = dummy_majority.predict(X_eval)\n",
    "    \n",
    "    # Random baseline\n",
    "    dummy_random = DummyClassifier(strategy='uniform', random_state=42)\n",
    "    dummy_random.fit(X_eval, y_eval)\n",
    "    dummy_pred_random = dummy_random.predict(X_eval)\n",
    "    \n",
    "    # Compare accuracies\n",
    "    model_accuracy = accuracy_score(y_eval, y_pred)\n",
    "    majority_accuracy = accuracy_score(y_eval, dummy_pred_majority)\n",
    "    random_accuracy = accuracy_score(y_eval, dummy_pred_random)\n",
    "    \n",
    "    print(\"Model Comparison:\")\n",
    "    print(f\"  Our Model:      {model_accuracy:.4f}\")\n",
    "    print(f\"  Majority Class: {majority_accuracy:.4f}\")\n",
    "    print(f\"  Random:         {random_accuracy:.4f}\")\n",
    "    \n",
    "    improvement_over_majority = (model_accuracy - majority_accuracy) / majority_accuracy * 100\n",
    "    improvement_over_random = (model_accuracy - random_accuracy) / random_accuracy * 100\n",
    "    \n",
    "    print(f\"\\nImprovement:\")\n",
    "    print(f\"  vs Majority: +{improvement_over_majority:.1f}%\")\n",
    "    print(f\"  vs Random:   +{improvement_over_random:.1f}%\")\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    models = ['Random', 'Majority Class', 'Our Model']\n",
    "    accuracies = [random_accuracy, majority_accuracy, model_accuracy]\n",
    "    colors = ['lightcoral', 'lightblue', 'lightgreen']\n",
    "    \n",
    "    bars = plt.bar(models, accuracies, color=colors, edgecolor='black')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Model Performance Comparison')\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, acc in zip(bars, accuracies):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                f'{acc:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model is not None:\n",
    "    # Identify misclassified examples\n",
    "    misclassified = (y_pred != y_eval)\n",
    "    \n",
    "    print(f\"Total misclassified examples: {misclassified.sum()} out of {len(y_eval)} ({misclassified.mean()*100:.1f}%)\")\n",
    "    \n",
    "    if misclassified.sum() > 0:\n",
    "        # Analyze misclassified examples\n",
    "        false_positives = (y_pred == 1) & (y_eval == 0)\n",
    "        false_negatives = (y_pred == 0) & (y_eval == 1)\n",
    "        \n",
    "        print(f\"False Positives: {false_positives.sum()}\")\n",
    "        print(f\"False Negatives: {false_negatives.sum()}\")\n",
    "        \n",
    "        # Look at confidence of misclassified examples\n",
    "        if false_positives.sum() > 0:\n",
    "            fp_confidence = y_pred_proba[false_positives]\n",
    "            print(f\"False Positive confidence - Mean: {fp_confidence.mean():.3f}, Std: {fp_confidence.std():.3f}\")\n",
    "        \n",
    "        if false_negatives.sum() > 0:\n",
    "            fn_confidence = 1 - y_pred_proba[false_negatives]  # Confidence in negative prediction\n",
    "            print(f\"False Negative confidence - Mean: {fn_confidence.mean():.3f}, Std: {fn_confidence.std():.3f}\")\n",
    "        \n",
    "        # Plot confidence distribution for errors\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        if false_positives.sum() > 0:\n",
    "            plt.hist(y_pred_proba[false_positives], bins=20, alpha=0.7, label='False Positives')\n",
    "        if false_negatives.sum() > 0:\n",
    "            plt.hist(y_pred_proba[false_negatives], bins=20, alpha=0.7, label='False Negatives')\n",
    "        plt.xlabel('Prediction Probability')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title('Confidence Distribution of Errors')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Threshold analysis\n",
    "        plt.subplot(1, 2, 2)\n",
    "        thresholds = np.linspace(0, 1, 100)\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        \n",
    "        for thresh in thresholds:\n",
    "            pred_thresh = (y_pred_proba >= thresh).astype(int)\n",
    "            if pred_thresh.sum() > 0:  # Avoid division by zero\n",
    "                precisions.append(precision_score(y_eval, pred_thresh, zero_division=0))\n",
    "                recalls.append(recall_score(y_eval, pred_thresh, zero_division=0))\n",
    "            else:\n",
    "                precisions.append(0)\n",
    "                recalls.append(0)\n",
    "        \n",
    "        plt.plot(thresholds, precisions, label='Precision', linewidth=2)\n",
    "        plt.plot(thresholds, recalls, label='Recall', linewidth=2)\n",
    "        plt.axvline(x=0.5, color='red', linestyle='--', alpha=0.7, label='Default threshold')\n",
    "        plt.xlabel('Threshold')\n",
    "        plt.ylabel('Score')\n",
    "        plt.title('Precision-Recall vs Threshold')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Perfect predictions - no errors to analyze!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model is not None:\n",
    "    print(\"=\" * 50)\n",
    "    print(\"MODEL EVALUATION SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(f\"Model Type: {type(model).__name__}\")\n",
    "    print(f\"Evaluation Dataset Size: {len(y_eval)} samples\")\n",
    "    print(f\"\\nPerformance Metrics:\")\n",
    "    print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall:    {recall:.4f}\")\n",
    "    print(f\"  F1-Score:  {f1:.4f}\")\n",
    "    print(f\"  ROC-AUC:   {roc_auc:.4f}\")\n",
    "    \n",
    "    print(f\"\\nRecommendations:\")\n",
    "    \n",
    "    if roc_auc < 0.7:\n",
    "        print(\"  ðŸ”´ Model performance is poor. Consider:\")\n",
    "        print(\"     - Feature engineering\")\n",
    "        print(\"     - Different algorithms\")\n",
    "        print(\"     - More data collection\")\n",
    "    elif roc_auc < 0.8:\n",
    "        print(\"  ðŸŸ¡ Model performance is fair. Consider:\")\n",
    "        print(\"     - Hyperparameter tuning\")\n",
    "        print(\"     - Feature selection\")\n",
    "        print(\"     - Ensemble methods\")\n",
    "    else:\n",
    "        print(\"  ðŸŸ¢ Model performance is good!\")\n",
    "    \n",
    "    if precision < 0.7:\n",
    "        print(\"  ðŸ“ˆ Low precision - consider increasing prediction threshold\")\n",
    "    \n",
    "    if recall < 0.7:\n",
    "        print(\"  ðŸ“‰ Low recall - consider decreasing prediction threshold\")\n",
    "    \n",
    "    print(f\"\\nNext Steps:\")\n",
    "    print(\"  1. Validate on additional test data\")\n",
    "    print(\"  2. Monitor model performance in production\")\n",
    "    print(\"  3. Set up model retraining pipeline\")\n",
    "    print(\"  4. Implement A/B testing for model updates\")\nelse:\n",
    "    print(\"No model evaluation performed. Please train a model first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}